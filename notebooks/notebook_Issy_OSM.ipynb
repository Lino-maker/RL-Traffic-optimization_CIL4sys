{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks from OpenStreetMap\n",
    "\n",
    "- import networks from OpenStreetMap.\n",
    "- integrate it and run it in Flow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the TestEnv environment is used to simply simulate the network\n",
    "from flow.envs import TestEnv\n",
    "\n",
    "# the Experiment class is used for running simulations\n",
    "from flow.core.experiment import Experiment\n",
    "\n",
    "# all other imports are standard\n",
    "from flow.core.params import VehicleParams\n",
    "from flow.core.params import NetParams, SumoCarFollowingParams\n",
    "from flow.core.params import InitialConfig\n",
    "from flow.core.params import EnvParams\n",
    "from flow.core.params import SumoParams\n",
    "\n",
    "from flow.networks import Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ajouter un flux de voiture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import InFlows\n",
    "\n",
    "inflow = InFlows()\n",
    "inflow.add(veh_type=\"human\",\n",
    "           edge=\"4794817\",\n",
    "           vehs_per_hour=100,\n",
    "           depart_speed=10,\n",
    "           color=\"green\")\n",
    "inflow.add(veh_type=\"human\",\n",
    "            edge=\"4783299#0\",\n",
    "            vehs_per_hour=100,\n",
    "            depart_lane=\"random\",\n",
    "            depart_speed=\"random\",\n",
    "            color=\"red\")\n",
    "inflow.add(veh_type=\"human\",\n",
    "           edge=\"-100822066\",\n",
    "           probability= 0.1,\n",
    "           depart_lane= 1,  # left lane\n",
    "           depart_speed= \"max\",\n",
    "           begin= 60,  # 1 minute\n",
    "           number= 30,\n",
    "           color= \"white\")\n",
    "inflow.add(veh_type=\"human\",\n",
    "            edge=\"155558218\",\n",
    "            period=2,\n",
    "            depart_lane=\"random\",\n",
    "            depart_speed=\"random\",\n",
    "            color=\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Créer des itinéraires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T21:56:40.239317Z",
     "start_time": "2020-03-10T21:56:40.156358Z"
    }
   },
   "outputs": [],
   "source": [
    "# Specifie les noms des edges du network dont les vehicules peuvent être originaire\n",
    "EDGES_DISTRIBUTION = [\n",
    "    \"-100822066\",\n",
    "    \"4794817\",\n",
    "    \"4783299#0\",\n",
    "    \"155558218\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# créer une nouvelle classe Network pour spécifier les itinéraires possibles\n",
    "class IssyOSMNetwork(Network):\n",
    "\n",
    "    def specify_routes(self, net_params):\n",
    "        return {\n",
    "            \"-100822066\": [ #N\n",
    "                \"-100822066\",\n",
    "                \"-352962858#1\",\n",
    "                \"-352962858#0\",\n",
    "                \"-4786940#1\",\n",
    "                 \"-4786940#0\",\n",
    "            ],\n",
    "            \n",
    "            \"4794817\" : [ #Loop\n",
    "                \"4794817\",\n",
    "                \"4786972#0\",\n",
    "                \"4786972#1\",\n",
    "                \"4786972#2\",\n",
    "                \"4786965#1\",\n",
    "                \"4786965#2\",\n",
    "                \"4786965#3\",\n",
    "                \"4795729\",\n",
    "                \"-352962858#1\",\n",
    "                \"4795742#0\",\n",
    "                \"4795742#1\",\n",
    "                \"4786965#3\",\n",
    "                \"4786965#4\",\n",
    "                \"4786965#5\",\n",
    "            ],\n",
    "            \n",
    "            \"4783299#0\": [    #E\n",
    "                \"4783299#0\",\n",
    "                \"4783299#1\",\n",
    "                \"4783299#2\",\n",
    "                \"4783299#3\",\n",
    "                \"4783299#4\",\n",
    "                \"4783299#5\",\n",
    "                \"4783299#6\",\n",
    "                \"4786940#0\",\n",
    "                \"4786940#1\",\n",
    "                \"352962858#0\",\n",
    "                \"352962858#1\",\n",
    "                \"100822066\",\n",
    "            ],\n",
    "            \n",
    "            \"155558218\": [\n",
    "                \"155558218\",\n",
    "                \"4786940#1\",\n",
    "                \"352962858#0\",\n",
    "                \"352962858#1\",\n",
    "                \"100822066\",\n",
    "            ],     \n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personnaliser un Environnement pour le RL\n",
    "\n",
    "More accessor objects and methods can be found within the Flow documentation at: http://berkeleyflow.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the base environment class\n",
    "from flow.envs import Env\n",
    "from gym.spaces.box import Box\n",
    "from gym.spaces import Tuple\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## definition de la classe environnement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myEnv(Env):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fonction action_space\n",
    "2 actions possibles pour chaque véhicule RL : +1 acceleration ou -1 acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myEnv(myEnv): # update my environment class\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        num_actions = self.initial_vehicles.num_rl_vehicles\n",
    "        accel_ub = self.env_params.additional_params[\"max_accel\"]\n",
    "        accel_lb = - abs(self.env_params.additional_params[\"max_decel\"])\n",
    "\n",
    "        return Box(low=accel_lb,\n",
    "                   high=accel_ub,\n",
    "                   shape=(num_actions,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## observation_space\n",
    "2 valeurs observé pour chaque véhicule: sa **position** et sa **vitesse**. En conséquence, nous avons besoin d'un espace d'observation qui est *deux fois plus grand que le nombre de véhicules* dans le network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myEnv(myEnv):  # update my environment class\n",
    "\n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        return Box(\n",
    "            low=0,\n",
    "            high=float(\"inf\"),\n",
    "            shape=(2*self.initial_vehicles.num_vehicles,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply_rl_actions\n",
    "`apply_rl_actions` : transforme les commandes de l'agent RL en actions réelles du simulateur.  \n",
    "\n",
    "Pour notre exemple, l'agent RL peut spécifier que les accélérations des véhicules RL avec la fonction **apply_acceleration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myEnv(myEnv):  # update my environment class\n",
    "\n",
    "    def _apply_rl_actions(self, rl_actions):\n",
    "        # the names of all autonomous (RL) vehicles in the network\n",
    "        rl_ids = self.k.vehicle.get_rl_ids()\n",
    "\n",
    "        # use the base environment method to convert actions into accelerations for the rl vehicles\n",
    "        self.k.vehicle.apply_acceleration(rl_ids, rl_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T08:46:36.345321Z",
     "start_time": "2020-03-11T08:46:36.334589Z"
    }
   },
   "source": [
    "## get_state\n",
    "\n",
    "`get_state` : extrait des features de l'environnement et fournit ensuite des entrées à la stratégie fournie par l'agent RL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myEnv(myEnv):  # update my environment class\n",
    "\n",
    "    def get_state(self, **kwargs):\n",
    "        # the get_ids() method is used to get the names of all vehicles in the network\n",
    "        ids = self.k.vehicle.get_ids()\n",
    "\n",
    "        # we use the get_absolute_position method to get the positions of all vehicles\n",
    "        pos = [self.k.vehicle.get_x_by_id(veh_id) for veh_id in ids]\n",
    "\n",
    "        # we use the get_speed method to get the velocities of all vehicles\n",
    "        vel = [self.k.vehicle.get_speed(veh_id) for veh_id in ids]\n",
    "\n",
    "        # the speeds and positions are concatenated to produce the state\n",
    "        return np.concatenate((pos, vel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute_reward\n",
    "\n",
    "`compute_reward` : renvoie la récompense associée à un état donné. \n",
    "\n",
    "Ici, la fonction de récompense est la **vitesse moyenne de tous les véhicules actuellement sur le réseau**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myEnv(myEnv):  # update my environment class\n",
    "\n",
    "    def compute_reward(self, rl_actions, **kwargs):\n",
    "        # the get_ids() method is used to get the names of all vehicles in the network\n",
    "        ids = self.k.vehicle.get_ids()\n",
    "\n",
    "        # we next get a list of the speeds of all vehicles in the network\n",
    "        speeds = self.k.vehicle.get_speed(ids)\n",
    "\n",
    "        # finally, we return the average of all these speeds as the reward\n",
    "        return np.mean(speeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "HORIZON = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************\n",
      "**********************************************************\n",
      "**********************************************************\n",
      "WARNING: Inflows will cause computational performance to\n",
      "significantly decrease after large number of rollouts. In \n",
      "order to avoid this, set SumoParams(restart_instance=True).\n",
      "**********************************************************\n",
      "**********************************************************\n",
      "**********************************************************\n",
      "Round 0, return: 6253.733328946158\n",
      "Average, std return:    6253.733328946158, 0.0\n",
      "Average, std speed:     6.25373332894616, 0.0\n",
      "Total time:             29.722063541412354\n",
      "steps/second:           39.5910501297408\n",
      "vehicles.steps/second:  756.9930875054912\n"
     ]
    }
   ],
   "source": [
    "from flow.controllers import IDMController, ContinuousRouter\n",
    "from flow.core.experiment import Experiment\n",
    "from flow.core.params import SumoParams, EnvParams, InitialConfig, NetParams\n",
    "from flow.core.params import VehicleParams\n",
    "from flow.networks.ring import RingNetwork, ADDITIONAL_NET_PARAMS\n",
    "\n",
    "ADDITIONAL_ENV_PARAMS = {\"max_accel\": 1, \"max_decel\": 1}\n",
    "\n",
    "# SUMO PARAM\n",
    "sumo_params = SumoParams(sim_step=0.1, render=True)\n",
    "\n",
    "# create VEHICLE\n",
    "vehicles = VehicleParams()\n",
    "vehicles.add(veh_id=\"human\",\n",
    "             acceleration_controller=(IDMController, {}),\n",
    "             num_vehicles=22)\n",
    "\n",
    "# ENVIRONMENT PARAM\n",
    "env_params = EnvParams(additional_params=ADDITIONAL_ENV_PARAMS, horizon=HORIZON)\n",
    "\n",
    "# NETWORK PARAM\n",
    "additional_net_params = ADDITIONAL_NET_PARAMS.copy()\n",
    "net_params = NetParams(additional_params=additional_net_params, inflows=inflow, osm_path='issy.osm')\n",
    "\n",
    "# NETWORK\n",
    "network = IssyOSMNetwork\n",
    "\n",
    "# INITIAL CONFIG\n",
    "initial_config = InitialConfig(edges_distribution=EDGES_DISTRIBUTION) #spacing=\"random\",\n",
    "\n",
    "# dictionnaire FLOW_PARAM\n",
    "flow_params = dict( exp_tag  = 'ISSY_RL_test',\n",
    "                    env_name = myEnv,  # using my new environment for the simulation\n",
    "                    network  = network,\n",
    "                    simulator='traci',\n",
    "                    sim      = sumo_params,\n",
    "                    env      = env_params,\n",
    "                    net      = net_params,\n",
    "                    veh      = vehicles,\n",
    "                    initial  = initial_config)\n",
    "\n",
    "# create EXPERIMENT with class created\n",
    "exp = Experiment(flow_params)\n",
    "\n",
    "# RUN SIMULATION SUMO\n",
    "_ = exp.run(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Pour qu'un environnement puisse être entrainé, l'environnement doit être accessible via l'importation à partir de flow.envs. <font color='red'> On copie alors l'environnement créé dans un fichier .py et on importe l'environnement dans `flow.envs.__init__.py`. </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: only runs if the above procedure have been performed\n",
    "from flow.envs import myEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ray\n",
    "from ray.rllib.agents.registry import get_agent_class\n",
    "from ray.tune import run_experiments\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "# from flow.networks.ring import RingNetwork, ADDITIONAL_NET_PARAMS\n",
    "from flow.utils.registry import make_create_env\n",
    "from flow.utils.rllib import FlowParamsEncoder\n",
    "from flow.core.params import VehicleParams, SumoCarFollowingParams\n",
    "from flow.controllers import RLController, IDMController, ContinuousRouter\n",
    "\n",
    "\n",
    "# number of rollouts per training iteration\n",
    "N_ROLLOUTS = 20\n",
    "# number of parallel workers\n",
    "N_CPUS = 2\n",
    "\n",
    "\n",
    "# SUMO PARAM\n",
    "sumo_params = SumoParams(sim_step=0.1, render=False)\n",
    "\n",
    "# create VEHICLE : 1 RL and 21 humans (simulated comportement)\n",
    "vehicles = VehicleParams()\n",
    "vehicles.add(veh_id=\"rl\", acceleration_controller=(RLController, {}), num_vehicles=1)\n",
    "vehicles.add(veh_id=\"human\", acceleration_controller=(IDMController, {}), num_vehicles=21)\n",
    "\n",
    "# ENVIRONMENT PARAM\n",
    "env_params = EnvParams(horizon=HORIZON)\n",
    "\n",
    "# NETWORK PARAM\n",
    "net_params = NetParams()\n",
    "\n",
    "# NETWORK\n",
    "network = IssyOSMNetwork\n",
    "\n",
    "# INITIAL CONFIG\n",
    "initial_config = InitialConfig()\n",
    "\n",
    "flow_params = dict( exp_tag   = \"ISSY_RL_train\", \n",
    "                    env_name  = myEnv,  \n",
    "                    network   = IssyOSMNetwork,\n",
    "                    simulator = 'traci', # simulator that is used by the experiment\n",
    "                    sim       = sumo_params,\n",
    "                    env       = env_params,\n",
    "                    net       = net_params,\n",
    "                    veh       = vehicles,\n",
    "                    initial   = initial_config)\n",
    "\n",
    "def setup_exps():\n",
    "    \"\"\"Return the relevant components of an RLlib experiment.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        name of the training algorithm\n",
    "    str\n",
    "        name of the gym environment to be trained\n",
    "    dict\n",
    "        training configuration parameters\n",
    "    \"\"\"\n",
    "    alg_run = \"PPO\"\n",
    "    agent_cls = get_agent_class(alg_run)\n",
    "    config = agent_cls._default_config.copy()\n",
    "    config[\"num_workers\"] = N_CPUS\n",
    "    config[\"train_batch_size\"] = HORIZON * N_ROLLOUTS\n",
    "    config[\"gamma\"] = 0.999  # discount rate\n",
    "    config[\"model\"].update({\"fcnet_hiddens\": [3, 3]})\n",
    "    config[\"use_gae\"] = True\n",
    "    config[\"lambda\"] = 0.97\n",
    "    config[\"kl_target\"] = 0.02\n",
    "    config[\"num_sgd_iter\"] = 10\n",
    "    config['clip_actions'] = False  # FIXME(ev) temporary ray bug\n",
    "    config[\"horizon\"] = HORIZON\n",
    "\n",
    "    # save the flow params for replay\n",
    "    flow_json = json.dumps( flow_params, cls=FlowParamsEncoder, sort_keys=True, indent=4)\n",
    "    config['env_config']['flow_params'] = flow_json\n",
    "    config['env_config']['run'] = alg_run\n",
    "\n",
    "    create_env, gym_name = make_create_env(params=flow_params, version=0)\n",
    "\n",
    "    # Register as rllib env\n",
    "    register_env(gym_name, create_env)\n",
    "    \n",
    "    return alg_run, gym_name, config\n",
    "\n",
    "\n",
    "alg_run, gym_name, config = setup_exps()\n",
    "\n",
    "ray.init(num_cpus=N_CPUS + 1)\n",
    "\n",
    "trials = run_experiments({\n",
    "    flow_params[\"exp_tag\"]: {\n",
    "        \"run\": alg_run,\n",
    "        \"env\": gym_name,\n",
    "        \"config\": {\n",
    "            **config\n",
    "        },\n",
    "        \"checkpoint_freq\": 20,\n",
    "        \"checkpoint_at_end\": True,\n",
    "        \"max_failures\": 999,\n",
    "        \"stop\": {\n",
    "            \"training_iteration\": 200,\n",
    "        },\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# /!\\ Voir tutoriel 10 pour controle des feux"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
