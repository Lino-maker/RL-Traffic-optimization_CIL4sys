{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks from OpenStreetMap\n",
    "\n",
    "\n",
    "- crée un network à partir d'un fichier .osm et des trajectoires de véhiculess\n",
    "- ajoute un flux de voiture sur les routes\n",
    "- personnalise un Environnement pour le RL\n",
    "- integrate it and run it in Flow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import VehicleParams\n",
    "from flow.core.params import NetParams, SumoCarFollowingParams\n",
    "from flow.core.params import InitialConfig\n",
    "from flow.core.params import EnvParams\n",
    "from flow.core.params import SumoParams\n",
    "from flow.controllers import RLController, IDMController\n",
    "from flow.networks import Network\n",
    "from flow.core.params import InFlows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crée le network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifie les noms des routes du network dont les vehicules peuvent être s'insérer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDGES_DISTRIBUTION = [\n",
    "    \"-100822066\",\n",
    "    \"4794817\",\n",
    "    \"4783299#0\",\n",
    "    \"155558218\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "créer la classe Network pour spécifier les itinéraires possibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IssyOSMNetwork(Network):\n",
    "\n",
    "    def specify_routes(self, net_params):\n",
    "        return {\n",
    "            \"-100822066\": [ #N\n",
    "                \"-100822066\",\n",
    "                \"-352962858#1\",\n",
    "                \"-352962858#0\",\n",
    "                \"-4786940#1\",\n",
    "                 \"-4786940#0\",\n",
    "            ],\n",
    "            \n",
    "            \"4794817\" : [ #Loop\n",
    "                \"4794817\",\n",
    "                \"4786972#0\",\n",
    "                \"4786972#1\",\n",
    "                \"4786972#2\",\n",
    "                \"4786965#1\",\n",
    "                \"4786965#2\",\n",
    "                \"4786965#3\",\n",
    "                \"4795729\",\n",
    "                \"-352962858#1\",\n",
    "                \"4795742#0\",\n",
    "                \"4795742#1\",\n",
    "                \"4786965#3\",\n",
    "                \"4786965#4\",\n",
    "                \"4786965#5\",\n",
    "            ],\n",
    "            \n",
    "            \"4783299#0\": [    #E\n",
    "                \"4783299#0\",\n",
    "                \"4783299#1\",\n",
    "                \"4783299#2\",\n",
    "                \"4783299#3\",\n",
    "                \"4783299#4\",\n",
    "                \"4783299#5\",\n",
    "                \"4783299#6\",\n",
    "                \"4786940#0\",\n",
    "                \"4786940#1\",\n",
    "                \"352962858#0\",\n",
    "                \"352962858#1\",\n",
    "                \"100822066\",\n",
    "            ],\n",
    "            \n",
    "            \"155558218\": [\n",
    "                \"155558218\",\n",
    "                \"4786940#1\",\n",
    "                \"352962858#0\",\n",
    "                \"352962858#1\",\n",
    "                \"100822066\",\n",
    "            ],     \n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajoute les flux de voiture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`IDMController` : The Intelligent Driver Model is a car-following model specifying vehicle dynamics by a differential equation for acceleration $\\dot{v}$.\n",
    "\n",
    "`RLController` : a trainable autuonomous vehicle whose actions are dictated by an RL agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles = VehicleParams()\n",
    "vehicles.add(\"human\",acceleration_controller=(IDMController, {}))\n",
    "vehicles.add(\"rl\",acceleration_controller=(RLController, {}),num_vehicles=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `vehs_per_hour`: nombre de vehicule par heure, uniformément espacés. Par exemple, comme il y a $60 \\times 60 = 3600$ secondes dans une heure, le parametre $\\frac{3600}{5}=720$ va faire rentrer des vehicules dans le network toutes les $5$ secondes.\n",
    "\n",
    "- `probability`: c'est la probabilité qu'un véhicule entre dans le network toutes les secondes. Par exemple, si on la fixe à $0.2$, alors chaque seconde de la simulation un véhicule aura $\\frac{1}{5}$ chance d'entrer dans le network\n",
    "\n",
    "- `period`: C'est le temps en secondes entre 2 véhicules qui sont insérés. Par exemple, le fixer à $5$ ferait rentrer des véhicules dans le network toutes les $5$ secondes (ce qui équivaut à mettre `vehs_per_hour` à $720$).\n",
    "\n",
    "<font color='red'>\n",
    "$\\rightarrow$ Exactement 1 seul de ces 3 paramètres doit être configurer !\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflow = InFlows()\n",
    "\n",
    "inflow.add(veh_type      = \"human\",\n",
    "           edge          = \"4794817\",\n",
    "           probability   = 0.3, \n",
    "           depart_speed  = 7,\n",
    "           depart_lane   = \"random\")\n",
    "\n",
    "inflow.add(veh_type      = \"human\",\n",
    "           edge          = \"4783299#0\",\n",
    "           probability   = 0.2,\n",
    "           depart_speed  = 7,\n",
    "           depart_lane   = \"random\")\n",
    "\n",
    "inflow.add(veh_type       = \"human\",\n",
    "           edge           = \"-100822066\",\n",
    "           probability    = 0.25,\n",
    "           depart_speed   = 7,\n",
    "           depart_lane    = \"random\")\n",
    "\n",
    "inflow.add(veh_type       = \"rl\",\n",
    "           edge           = \"-100822066\",\n",
    "           probability    = 0.05,\n",
    "           depart_speed   = 7,\n",
    "           depart_lane    = \"random\",\n",
    "           color          = \"blue\")\n",
    "\n",
    "inflow.add(veh_type       = \"human\",\n",
    "           edge          = \"155558218\",\n",
    "           probability   = 0.2,\n",
    "           depart_speed  = 7,\n",
    "           depart_lane   = \"random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personnalise un Environnement pour le RL\n",
    "\n",
    "plus de méthodes sur : http://berkeleyflow.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the base environment class\n",
    "from flow.envs import Env\n",
    "from gym.spaces.box import Box\n",
    "from gym.spaces import Tuple\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### definition de la classe environnement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myEnv(Env):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fonction action_space\n",
    "2 actions possibles pour chaque véhicule RL : +1 acceleration ou -1 acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myEnv(myEnv): # update my environment class\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        num_actions = self.initial_vehicles.num_rl_vehicles\n",
    "        accel_ub = self.env_params.additional_params[\"max_accel\"]\n",
    "        accel_lb = - abs(self.env_params.additional_params[\"max_decel\"])\n",
    "\n",
    "        return Box(low=accel_lb,\n",
    "                   high=accel_ub,\n",
    "                   shape=(num_actions,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### observation_space\n",
    "2 valeurs observé pour chaque véhicule: sa **position** et sa **vitesse**. En conséquence, nous avons besoin d'un espace d'observation qui est *deux fois plus grand que le nombre de véhicules* dans le network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myEnv(myEnv):  # update my environment class\n",
    "\n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        return Box(\n",
    "            low=0,\n",
    "            high=float(\"inf\"),\n",
    "            shape=(2*self.initial_vehicles.num_vehicles,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply_rl_actions\n",
    "`apply_rl_actions` : transforme les commandes de l'agent RL en actions réelles du simulateur.  \n",
    "\n",
    "Pour notre exemple, l'agent RL peut spécifier que les accélérations des véhicules RL avec la fonction **apply_acceleration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myEnv(myEnv):  # update my environment class\n",
    "\n",
    "    def _apply_rl_actions(self, rl_actions):\n",
    "        # the names of all autonomous (RL) vehicles in the network\n",
    "        rl_ids = self.k.vehicle.get_rl_ids()\n",
    "\n",
    "        # use the base environment method to convert actions into accelerations for the rl vehicles\n",
    "        self.k.vehicle.apply_acceleration(rl_ids, rl_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T08:46:36.345321Z",
     "start_time": "2020-03-11T08:46:36.334589Z"
    }
   },
   "source": [
    "### get_state\n",
    "\n",
    "`get_state` : extrait des features de l'environnement et fournit ensuite des entrées à la stratégie fournie par l'agent RL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myEnv(myEnv):  # update my environment class\n",
    "\n",
    "    def get_state(self, **kwargs):\n",
    "        # the get_ids() method is used to get the names of all vehicles in the network\n",
    "        ids = self.k.vehicle.get_ids()\n",
    "\n",
    "        # we use the get_absolute_position method to get the positions of all vehicles\n",
    "        pos = [self.k.vehicle.get_x_by_id(veh_id) for veh_id in ids]\n",
    "\n",
    "        # we use the get_speed method to get the velocities of all vehicles\n",
    "        vel = [self.k.vehicle.get_speed(veh_id) for veh_id in ids]\n",
    "\n",
    "        # the speeds and positions are concatenated to produce the state\n",
    "        return np.concatenate((pos, vel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute_reward\n",
    "\n",
    "`compute_reward` : renvoie la récompense associée à un état donné. \n",
    "\n",
    "Ici, la fonction de récompense est la **vitesse moyenne de tous les véhicules actuellement sur le réseau**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myEnv(myEnv):  # update my environment class\n",
    "\n",
    "    def compute_reward(self, rl_actions, **kwargs):\n",
    "        # the get_ids() method is used to get the names of all vehicles in the network\n",
    "        ids = self.k.vehicle.get_ids()\n",
    "\n",
    "        # we next get a list of the speeds of all vehicles in the network\n",
    "        speeds = self.k.vehicle.get_speed(ids)\n",
    "\n",
    "        # finally, we return the average of all these speeds as the reward\n",
    "        return np.mean(speeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lance une simulation avec Training RLlib\n",
    "\n",
    "Pour qu'un environnement puisse être entrainé, l'environnement doit être accessible via l'importation à partir de flow.envs. \n",
    "\n",
    "\n",
    "<font color='red'>\n",
    "Copier l'environnement créé dans un fichier .py et on importe l'environnement dans `flow.envs.__init__.py`.\n",
    "Mettre le chemin absolu du fichier .osm .\n",
    "</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ray\n",
    "from ray.rllib.agents.registry import get_agent_class\n",
    "from ray.tune import run_experiments\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from flow.networks.ring import ADDITIONAL_NET_PARAMS\n",
    "from flow.utils.registry import make_create_env\n",
    "from flow.utils.rllib import FlowParamsEncoder\n",
    "from flow.core.params import VehicleParams, SumoCarFollowingParams\n",
    "\n",
    "#from flow.envs import AccelEnv as myEnv\n",
    "#ADDITIONAL_ENV_PARAMS = {\"target_velocity\": 20,\"sort_vehicles\": False,\"max_accel\": 1,\"max_decel\": 1}\n",
    "from myenv import myEnv as myEnv\n",
    "ADDITIONAL_ENV_PARAMS = {\"max_accel\": 1, \"max_decel\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T13:09:50.179116Z",
     "start_time": "2020-03-24T13:09:49.899185Z"
    }
   },
   "outputs": [],
   "source": [
    "HORIZON = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T13:09:50.179116Z",
     "start_time": "2020-03-24T13:09:49.899185Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ray.rllib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-02a56adfb713>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregistry\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_agent_class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtune\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrun_experiments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtune\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregistry\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mregister_env\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ray.rllib'"
     ]
    }
   ],
   "source": [
    "# number of rollouts per training iteration\n",
    "N_ROLLOUTS = 10\n",
    "# number of parallel workers\n",
    "N_CPUS = 2\n",
    "\n",
    "# SUMO PARAM\n",
    "sumo_params = SumoParams(sim_step=0.1, render=False, restart_instance=True)\n",
    "\n",
    "# ENVIRONMENT PARAM\n",
    "env_params = EnvParams(additional_params=ADDITIONAL_ENV_PARAMS, horizon=HORIZON)\n",
    "\n",
    "# NETWORK PARAM\n",
    "additional_net_params = ADDITIONAL_NET_PARAMS.copy()\n",
    "path_file  = '/home/julien/projet_CIL4SYS/issy.osm'\n",
    "net_params = NetParams(inflows=inflow, osm_path=path_file) \n",
    "\n",
    "# NETWORK\n",
    "network = IssyOSMNetwork\n",
    "\n",
    "# INITIAL CONFIG\n",
    "initial_config = InitialConfig(edges_distribution=EDGES_DISTRIBUTION)\n",
    "\n",
    "vehicles = VehicleParams()\n",
    "vehicles.add(veh_id=\"human\", acceleration_controller=(IDMController, {}), num_vehicles=5)\n",
    "\n",
    "flow_params = dict( exp_tag   = \"ISSY_RL_train\", \n",
    "                    env_name  = myEnv,  \n",
    "                    network   = IssyOSMNetwork,\n",
    "                    simulator = 'traci', # simulator that is used by the experiment\n",
    "                    sim       = sumo_params,\n",
    "                    env       = env_params,\n",
    "                    net       = net_params,\n",
    "                    veh       = vehicles,\n",
    "                    initial   = initial_config)\n",
    "\n",
    "def setup_exps():\n",
    "    \"\"\"Return the relevant components of an RLlib experiment.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        name of the training algorithm\n",
    "    str\n",
    "        name of the gym environment to be trained\n",
    "    dict\n",
    "        training configuration parameters\n",
    "    \"\"\"\n",
    "    alg_run   = \"PPO\"\n",
    "    agent_cls = get_agent_class(alg_run)\n",
    "    config    = agent_cls._default_config.copy()\n",
    "    config[\"num_workers\"]      = N_CPUS\n",
    "    config[\"train_batch_size\"] = HORIZON * N_ROLLOUTS\n",
    "    config[\"gamma\"]            = 0.999  # discount rate\n",
    "    config[\"use_gae\"]          = True\n",
    "    config[\"lambda\"]           = 0.97\n",
    "    config[\"kl_target\"]        = 0.02\n",
    "    config[\"num_sgd_iter\"]     = 10\n",
    "    config['clip_actions']     = False  # FIXME(ev) temporary ray bug\n",
    "    config[\"horizon\"]          = HORIZON\n",
    "    config[\"model\"].update({\"fcnet_hiddens\": [3, 3]})\n",
    "    \n",
    "    # save the flow params for replay\n",
    "    flow_json = json.dumps( flow_params, cls=FlowParamsEncoder, sort_keys=True, indent=4)\n",
    "    config['env_config']['flow_params'] = flow_json\n",
    "    config['env_config']['run'] = alg_run\n",
    "\n",
    "    create_env, gym_name = make_create_env(params=flow_params, version=0)\n",
    "\n",
    "    # Register as rllib env\n",
    "    register_env(gym_name, create_env)\n",
    "    \n",
    "    return alg_run, gym_name, config\n",
    "\n",
    "\n",
    "alg_run, gym_name, config = setup_exps()\n",
    "\n",
    "ray.init(num_cpus=N_CPUS + 1)\n",
    "\n",
    "trials = run_experiments({\n",
    "    flow_params[\"exp_tag\"]: {\n",
    "        \"run\": alg_run,\n",
    "        \"env\": gym_name,\n",
    "        \"config\": {**config},\n",
    "        \"checkpoint_freq\": 500,\n",
    "        \"checkpoint_at_end\": True,\n",
    "        \"max_failures\": 999,\n",
    "        \"stop\": {\"training_iteration\": 5},\n",
    "    }\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
